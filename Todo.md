# ğŸ“‹ TODO LIST - MNIST Softmax Regression Project

## âœ… ÄÃ£ hoÃ n thÃ nh
- [x] Cáº¥u trÃºc thÆ° má»¥c project (BE, FE, MODELS)
- [x] Setup requirements.txt
- [x] Táº£i MNIST dataset tá»« Kaggle
- [x] Class base `SoftmaxRegression` (baseline Ä‘Æ¡n giáº£n)

---

## ğŸ¯ Cáº¦N LÃ€M TIáº¾P THEO

### ğŸ“ MODELS/models/utils/ (Báº¯t Ä‘áº§u tá»« Ä‘Ã¢y)

#### 1. **dataset.py** - Xá»­ lÃ½ dá»¯ liá»‡u MNIST
**Má»¥c tiÃªu**: Load vÃ  chuáº©n bá»‹ dá»¯ liá»‡u cho training

**TODO**:
- [ ] Viáº¿t class `MNISTDataset`:
  - [ ] Äá»c file `.idx3-ubyte` (images) vÃ  `.idx1-ubyte` (labels)
  - [ ] Parse binary format (magic number, dimensions, data)
  - [ ] Tráº£ vá» numpy arrays: `(N, 28, 28)` cho images, `(N,)` cho labels
  - [ ] Method `__len__()` vÃ  `__getitem__(idx)`

- [ ] Viáº¿t class `DataLoader`:
  - [ ] Chia data thÃ nh batches
  - [ ] Shuffle data má»—i epoch
  - [ ] Method `__iter__()` Ä‘á»ƒ duyá»‡t qua batches
  - [ ] Há»— trá»£ `batch_size` parameter

**Hint**: 
- MNIST binary format: magic number (4 bytes) â†’ dimensions â†’ pixel data
- DÃ¹ng `struct.unpack()` Ä‘á»ƒ Ä‘á»c binary
- Images: uint8 [0-255], cáº§n normalize vá» [0-1]

---

#### 2. **visualization.py** - Visualize dá»¯ liá»‡u vÃ  káº¿t quáº£
**Má»¥c tiÃªu**: Hiá»ƒn thá»‹ áº£nh, confusion matrix, training curves

**TODO**:
- [ ] Function `plot_samples(images, labels, predictions=None)`:
  - [ ] Hiá»ƒn thá»‹ grid áº£nh vá»›i matplotlib
  - [ ] Show true label vÃ  predicted label (náº¿u cÃ³)

- [ ] Function `plot_confusion_matrix(y_true, y_pred)`:
  - [ ] TÃ­nh confusion matrix
  - [ ] Váº½ heatmap vá»›i matplotlib/seaborn

- [ ] Function `plot_training_history(losses, accuracies)`:
  - [ ] Váº½ loss curve vÃ  accuracy curve
  - [ ] Subplot cho train vÃ  validation

---

### ğŸ“ MODELS/models/src/ (Core Models)

#### 3. **base.py** - Implement Softmax Regression
**Má»¥c tiÃªu**: HoÃ n thiá»‡n class base vá»›i NumPy thuáº§n

**TODO**:
- [ ] Method `fit(X, y)`:
  - [ ] Initialize weights vá»›i Xavier/He initialization
  - [ ] One-hot encode labels
  - [ ] Implement mini-batch gradient descent loop:
    ```
    for epoch in range(num_epochs):
        shuffle data
        for each batch:
            forward pass â†’ compute loss â†’ backward pass â†’ update weights
    ```
  - [ ] Forward pass: `z = X @ W + b`, `softmax(z)`
  - [ ] Loss: Cross-entropy + L2 regularization
  - [ ] Backward: Gradient cá»§a cross-entropy wrt W, b
  - [ ] Update: `W -= learning_rate * dW`

- [ ] Method `predict(X)`:
  - [ ] Forward pass
  - [ ] Return argmax cá»§a probabilities

- [ ] Method `predict_proba(X)`:
  - [ ] Return softmax probabilities

- [ ] Method `score(X, y)`:
  - [ ] Accuracy = mean(predictions == y)

**CÃ´ng thá»©c quan trá»ng**:
- Softmax: `softmax(z_i) = exp(z_i) / sum(exp(z_j))`
- Cross-entropy loss: `L = -mean(sum(y_true * log(y_pred)))`
- Gradient: `dL/dW = X.T @ (y_pred - y_true) / batch_size`

---

#### 4. **model_pixel.py** - Raw Pixel Model
**Má»¥c tiÃªu**: Model Ä‘Æ¡n giáº£n nháº¥t, dÃ¹ng pixel thÃ´

**TODO**:
- [ ] Inherit tá»« `SoftmaxRegression`
- [ ] Override method `preprocess_features(X)`:
  - [ ] Flatten: `(N, 28, 28) â†’ (N, 784)`
  - [ ] Normalize: `X / 255.0`
  - [ ] Return normalized features

---

#### 5. **model_edge.py** - Edge Detection Model
**Má»¥c tiÃªu**: TrÃ­ch xuáº¥t features báº±ng edge detection

**TODO**:
- [ ] Inherit tá»« `SoftmaxRegression`
- [ ] Override method `preprocess_features(X)`:
  - [ ] Loop qua tá»«ng áº£nh
  - [ ] Apply Sobel operator (cv2.Sobel):
    - Sobel X (vertical edges)
    - Sobel Y (horizontal edges)
    - Magnitude = sqrt(SxÂ² + SyÂ²)
  - [ ] Apply Canny edge detection (cv2.Canny)
  - [ ] Concatenate [sobel_mag, canny]
  - [ ] Flatten vÃ  normalize
  - [ ] Return edge features

**Hint**: 
- Sobel: Detect gradients â†’ highlight edges
- Canny: Complete edge detection algorithm
- Káº¿t há»£p cáº£ 2 Ä‘á»ƒ cÃ³ nhiá»u thÃ´ng tin hÆ¡n

---

#### 6. **model_pca.py** - PCA Dimensionality Reduction
**Má»¥c tiÃªu**: Giáº£m chiá»u dá»¯ liá»‡u tá»« 784 xuá»‘ng ~50-100 dimensions

**TODO**:
- [ ] Inherit tá»« `SoftmaxRegression`
- [ ] Method `fit_pca(X)`:
  - [ ] Center data: `X_centered = X - mean(X)`
  - [ ] Compute covariance matrix: `C = X_centered.T @ X_centered`
  - [ ] Eigenvalue decomposition: `eig_vals, eig_vecs = np.linalg.eigh(C)`
  - [ ] Sort eigenvectors by eigenvalues (descending)
  - [ ] Select top k components (preserve 95% variance)
  - [ ] Store `self.mean`, `self.components`

- [ ] Method `transform_pca(X)`:
  - [ ] Center: `X - self.mean`
  - [ ] Project: `X_centered @ self.components`
  - [ ] Return reduced features

- [ ] Override `preprocess_features(X)`:
  - [ ] Flatten vÃ  normalize
  - [ ] Apply PCA transform
  - [ ] Return reduced features

- [ ] Override `fit(X, y)`:
  - [ ] Fit PCA trÆ°á»›c
  - [ ] Gá»i `super().fit(X, y)`

**CÃ´ng thá»©c PCA**:
- Covariance: `C = (1/n) * X.T @ X`
- Explained variance: `eig_val / sum(eig_vals)`
- Transform: `X_new = (X - Î¼) @ V_k`

---

### ğŸ“ MODELS/ (Root level)

#### 7. **train.py** - Train táº¥t cáº£ models
**Má»¥c tiÃªu**: Script Ä‘á»ƒ train vÃ  save 3 models

**TODO**:
- [ ] Load MNIST dataset
- [ ] Split train/validation (náº¿u cáº§n)
- [ ] Train tá»«ng model:
  ```python
  pixel_model = PixelSoftmaxRegression(lr=0.1, epochs=500)
  pixel_model.fit(X_train, y_train)
  ```
- [ ] Evaluate trÃªn test set
- [ ] Save models vá»›i pickle: `pickle.dump(model, f)`
- [ ] Save vÃ o `trained/pixel_model.pkl`, etc.
- [ ] Print accuracy cá»§a tá»«ng model

---

### ğŸ“ BE/ (Backend API)

#### 8. **app.py** - Flask API
**Má»¥c tiÃªu**: API endpoint Ä‘á»ƒ predict tá»« FE

**TODO**:
- [ ] Load 3 trained models khi start server
- [ ] Route `POST /predict`:
  - [ ] Nháº­n base64 image tá»« frontend
  - [ ] Decode base64 â†’ PIL Image
  - [ ] Resize vá» 28x28 grayscale
  - [ ] Invert colors (canvas tráº¯ng â†’ MNIST Ä‘en)
  - [ ] Normalize pixel values
  - [ ] Call `model.predict_proba()` cho cáº£ 3 models
  - [ ] Return JSON:
    ```json
    {
      "pixel_model": {"prediction": 5, "probabilities": [...], "confidence": 0.95},
      "edge_model": {...},
      "pca_model": {...}
    }
    ```
- [ ] Route `GET /health`: Check models loaded
- [ ] Enable CORS

---

### ğŸ“ FE/ (Frontend UI)

#### 9. **script.js** - Drawing Canvas Logic
**Má»¥c tiÃªu**: Váº½ sá»‘ vÃ  gá»­i Ä‘áº¿n backend

**TODO**:
- [ ] Canvas drawing:
  - [ ] Mouse events: mousedown, mousemove, mouseup
  - [ ] Touch events cho mobile
  - [ ] Draw vá»›i `ctx.lineTo()` vÃ  `ctx.stroke()`
  - [ ] Brush size phÃ¹ há»£p (~15px)

- [ ] Clear button: Reset canvas vá» tráº¯ng

- [ ] Predict button:
  - [ ] Get canvas data: `canvas.toDataURL('image/png')`
  - [ ] Fetch POST `/predict` vá»›i base64 image
  - [ ] Parse response JSON
  - [ ] Display results

- [ ] Display results:
  - [ ] Show predicted digit (lá»›n, bold)
  - [ ] Show confidence score
  - [ ] Show probability bars cho 10 digits
  - [ ] Repeat cho cáº£ 3 models

---

#### 10. **style.css** - UI Styling
**TODO**:
- [ ] Canvas styling: border, cursor
- [ ] Button styles: hover effects
- [ ] Results layout: grid/flexbox
- [ ] Probability bars: height based on probability
- [ ] Responsive design cho mobile
- [ ] Color scheme: gradient background

---

#### 11. **index.html** - HTML Structure
**TODO**:
- [ ] Header: Title vÃ  instructions
- [ ] Canvas section vá»›i controls
- [ ] Results section (initially hidden)
- [ ] Link CSS vÃ  JS files

---

## ğŸ“ THá»¨ Tá»° KHUYÃŠN DÃ™NG

### Phase 1: Data Pipeline â­ (Báº®T Äáº¦U Tá»ª ÄÃ‚Y)
1. dataset.py - Äá»c vÃ  load MNIST
2. `visualization.py` - Xem data cÃ³ Ä‘Ãºng khÃ´ng
3. Test xem cÃ³ load Ä‘Æ°á»£c áº£nh + label khÃ´ng

### Phase 2: Core Model ğŸ§ 
4. `base.py` - Implement Softmax Regression
5. `model_pixel.py` - Test vá»›i pixel model trÆ°á»›c
6. Train 1 epoch xem loss cÃ³ giáº£m khÃ´ng

### Phase 3: Advanced Models ğŸš€
7. `model_edge.py` - Edge features
8. `model_pca.py` - PCA reduction
9. `train.py` - Train cáº£ 3 models Ä‘áº¿n há»™i tá»¥

### Phase 4: Backend ğŸ”§
10. `app.py` - Flask API
11. Test API vá»›i Postman/curl

### Phase 5: Frontend ğŸ¨
12. `script.js` + `index.html` + `style.css`
13. Test end-to-end flow

---

## ğŸ“š TÃ€I LIá»†U THAM KHáº¢O

**Softmax Regression**:
- Cross-entropy loss derivation
- Gradient descent with softmax
- NumPy vectorization tricks

**MNIST Binary Format**:
- http://yann.lecun.com/exdb/mnist/
- Magic numbers vÃ  byte order

**Edge Detection**:
- Sobel operator
- Canny edge detection
- OpenCV documentation

**PCA**:
- Eigenvalue decomposition
- Variance explained
- Dimensionality reduction

---

## ğŸ› DEBUG TIPS

- Print shapes thÆ°á»ng xuyÃªn: `print(X.shape)`
- Check numerical stability: softmax overflow â†’ subtract max
- Visualize intermediate results
- Start vá»›i small dataset Ä‘á»ƒ test nhanh
- Use `np.set_printoptions(precision=3)` Ä‘á»ƒ dá»… Ä‘á»c

---

**ChÃºc báº¡n code vui! ğŸš€**